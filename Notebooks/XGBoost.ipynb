{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Extreme Gradient Boosting is an **ensemble learning model** (combines multiple models to improve overall performance) from the *boosting* family (Each 'weak' model learns from the mistakes of the previous ones)\n",
    "\n",
    "Unlike *adaptive boosting*, which assigns weights to misclassified points, **gradient boosting** builds new models to predict the *residual errors* of the previous models and improves the predictions step by step. The name gradient boosting comes from the fact that the algorithm uses gradients (derivatives) to minimize the loss function. XGBoost minimizes a differentiable loss function using second-order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick note on the relationship between Residual Errors and Loss Function\n",
    "\n",
    "The **residual error** is nothing more than the difference between the *true value* of the target variable and the *prediction* made by the model:\n",
    "\n",
    "$$\n",
    "r_{i} = y_{i} - \\hat{y_{i}}\n",
    "$$\n",
    "\n",
    "A **loss function** is a *mathematical function* that **aggregates residual errors** into a single number often used to measure the model's performance. One example is the mean squared error (MSE) for regression:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum (y_{i} - \\hat{y_{i}})^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting fits a new model to predict *residuals* rather than the target variable directly. The residuals are computed from the gradient of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Mean Squared Error (MSE) and Residuals\n",
    "\n",
    "If the loss function is MSE:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum (y_{i} - \\hat{y_{i}})^2\n",
    "$$\n",
    "\n",
    "The gradient (derivative) with respect to predictions is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_{i}}} =  -2(y_{i} - \\hat{y_{i}})\n",
    "$$\n",
    "\n",
    "This shows that the gradient of the loss function is proportional to the negative residuals. Then, in gradient boosting, the next model is trained to predict the residual errors (or equivalently, follow the gradient of the loss function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data for taining and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DMatrix format \n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DMatrix is a special **data structure** used by XGBoost to optimize *training speed* and *memory usage*. It is more efficient than using raw NumPy arrays or pandas DataFrames because it handles missing values automatically, makes training faster by storing data in a compressed format and precomputes statistics like gradients to speed up calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task\n",
    "    'eval_metric': 'rmse',\n",
    "    'eta': 0.1,  # Learning rate\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "num_rounds = 100\n",
    "model = xgb.train(params, dtrain, num_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2260\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
